{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is dedicated to the application of SVD and subsequently feature extraction and clustering to the various datasets prepared in the data collection & cleaning notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are functions that will be used throughout the notebook. Their use is in the comments & will become more clear with application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to calculate the \"knee\" of a given function. It's application in this program is to calculate\n",
    "#the optimal value of k when performing truncated SVD\n",
    "def calculateKnee(values):\n",
    "    #get coordinates of all the points\n",
    "    nPoints = len(values)\n",
    "    allCoord = np.vstack((range(nPoints), values)).T\n",
    "    #np.array([range(nPoints), values])\n",
    "\n",
    "    # get the first point\n",
    "    firstPoint = allCoord[0]\n",
    "    # get vector between first and last point - this is the line\n",
    "    lineVec = allCoord[-1] - allCoord[0]\n",
    "    lineVecNorm = lineVec / np.sqrt(np.sum(lineVec**2))\n",
    "\n",
    "    # find the distance from each point to the line:\n",
    "    # vector between all points and first point\n",
    "    vecFromFirst = allCoord - firstPoint\n",
    "\n",
    "    # To calculate the distance to the line, we split vecFromFirst into two \n",
    "    # components, one that is parallel to the line and one that is perpendicular \n",
    "    # Then, we take the norm of the part that is perpendicular to the line and \n",
    "    # get the distance.\n",
    "    # We find the vector parallel to the line by projecting vecFromFirst onto \n",
    "    # the line. The perpendicular vector is vecFromFirst - vecFromFirstParallel\n",
    "    # We project vecFromFirst by taking the scalar product of the vector with \n",
    "    # the unit vector that points in the direction of the line (this gives us \n",
    "    # the length of the projection of vecFromFirst onto the line). If we \n",
    "    # multiply the scalar product by the unit vector, we have vecFromFirstParallel\n",
    "    scalarProduct = np.sum(vecFromFirst * np.matlib.repmat(lineVecNorm, nPoints, 1), axis=1)\n",
    "    vecFromFirstParallel = np.outer(scalarProduct, lineVecNorm)\n",
    "    vecToLine = vecFromFirst - vecFromFirstParallel\n",
    "\n",
    "    # distance to line is the norm of vecToLine\n",
    "    distToLine = np.sqrt(np.sum(vecToLine ** 2, axis=1))\n",
    "\n",
    "    # knee/elbow is the point with max distance value\n",
    "    idxOfBestPoint = np.argmax(distToLine)\n",
    "\n",
    "    print(\"Knee of the curve is at index =\",idxOfBestPoint)\n",
    "    return idxOfBestPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Master Matrix\n",
    "\n",
    "First we will apply SVD to our large dataframe - titled Master Dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in clean csv file \n",
    "master_df = pd.read_csv(\"C:/Users/rachh/OneDrive/Documents/Senior Thesis/master_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SVD\n",
    "\n",
    "Prior to applying truncated SVD, we will perform traditional svd to extract the singular values to determine the optimal rank to use when performced truncated SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert pandas dataframe objects to numpy arrays\n",
    "master_matrix = master_df.values\n",
    "#apply numpy SVD function to master matrix\n",
    "master_U,master_s,master_V = np.linalg.svd(master_matrix,full_matrices = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting SVD Results\n",
    "\n",
    "The size of each singular values (entries in s) tells you how much of the dataset's total variance is accounted for by each singular vector.\n",
    "\n",
    "The results of SVD can give us a matrix with lower rank that is said to approximate the original matrix. To reconstruct a lower dimensional matrix we must first select the top k largest singular values in s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting K \n",
    "\n",
    "* Plot singular values using a scree plot. A suitable cutoff point k is where the slope of the plot appears to flatten or where there is a detectable elbow or knee in the curve.\n",
    "* Calculate the contribution of each singular value by calculating the sum $f_k$ and the entropy of the dataset. The magnitude of the entropy indicates how many dimensions need to be retained "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Plot Singular Values with a Scree Plot\n",
    "\n",
    "A scree plot is simply a scatter plot in which the magnitudes of the singular values are plotted in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0,93),master_s)\n",
    "plt.title('Scree Plot of Singular Values: Master Matrix')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateKnee(master_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Calculate contribution of each value using $f_k$ and the entropy of the dataset. \n",
    "\n",
    "$f_k = s_k^2/\\sum_{i=1}^rs_i^2$ and $entropy = -1/\\log(r)\\sum_{k=1}^rf_k\\log(f_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare empty list to contain f_k values\n",
    "f_k_list = []\n",
    "s_2 = []\n",
    "\n",
    "for value in master_s:\n",
    "    s_2.append(value**2)\n",
    "\n",
    "s_i_sum = np.sum(s_2)\n",
    "for i in range(69):\n",
    "    s_k2 = s[i]**2\n",
    "    f_k = s_k2/s_i_sum\n",
    "    f_k_list.append(f_k)\n",
    "    \n",
    "log = -1*math.log(69,10)\n",
    "print(log)\n",
    "f_k_sum = 0\n",
    "for i in range(69):\n",
    "    f_k = f_k_list[i] \n",
    "    log_value = math.log(f_k,10)\n",
    "    f_k_sum = f_k * log_value\n",
    "\n",
    "print(log)\n",
    "print(f_k_sum)\n",
    "print(\"F_k values: \",f_k_list)\n",
    "print(\"Entropy: \", entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD\n",
    "\n",
    "Given the calculated k, we will perform truncated svd to produce an approximate master matrix with k features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sklearn's TruncatedSVD function to compute the SVD and compute the new approximated matrix. We can use this matrix to perform clustering algorithms in a lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 7)\n",
    "svd.fit(master_matrix)\n",
    "#Fit LSI model on training data X\n",
    "truncated_mmSVD = svd.transform(master_matrix)\n",
    "#Fit LSI model to X and perform dimensionality reduction on X.\n",
    "reducedTrunc_mmSVD = svd.fit_transform(master_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Sklearn's truncatedSVD function has built a built in explained variance ratio to evaluate model performance. In addition, we will also calculate the frobenius norm of the diffference between the original and approximate matrices(cite wherever I learned that this is something I might want to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The explained variance for the original LSI model is: \", truncated_mmSVD.explained_variance_ratio)\n",
    "print(\"The explained variance for the model with dimensionality reduction is: \" reducedTrunc_mmSVD.explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_approxDiff = np.subtract(master_matrix, truncated_mmSVD)\n",
    "mm_redApproxDiff = np.subtract(master_matrix, reducedTrun_mmSVD)\n",
    "\n",
    "print(\"The frobenius norm of the difference is: \", LA.norm(mm_approxDiff))\n",
    "print(\"The frobenius norm of the reduced difference is: \" LA.norm(mm_redApproxDiff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lab Data\n",
    "\n",
    "Given previous results, it is clear that far less features than available in the master dataframe are necessary to get a clear snapshot of the data. To narrow the scope of my analysis, I chose to perform a targeted analysis of first day lab data and mortality outcomes. The SVD computations performed above were condensed into easy to use functions to save time.\n",
    "\n",
    "Additionally, I performed truncatedSVD on standardized lab data that was later used for SDD calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateK(A):\n",
    "    #compute basic svd to find K value\n",
    "    U,s,V = np.linalg.svd(A,full_matrices)\n",
    "    k = calculateKnee(s)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in clean csv file\n",
    "lab_data = pd.read_csv(\"C:/Users/rachh/OneDrive/Documents/Senior Thesis/lab_data.csv\",low_memory = True)\n",
    "standardizedLab_df = pd.read_csv(\"C:/Users/rachh/OneDrive/Documents/Senior Thesis/standardizedLab_df.csv\", low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store dataframe values in numpy arrays\n",
    "lab_matrix = lab_data.values\n",
    "standardizedLabMatrix = standardizedLab_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply numpys SVD function (reduced version due to memory constraints)\n",
    "lab_k = calculateK(lab_matrix)\n",
    "standLab_k = calculatedK(standardizedLabMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the calculated values of k, a truncated SVD will be performed in a similar fashion to how it was done with the master data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labSVD = TruncatedSVD(n_components = lab_k)\n",
    "labSVD.fit(lab_matrix)\n",
    "#Fit LSI model on training data X\n",
    "truncated_labSVD = labSVD.transform(lab_matrix)\n",
    "#Fit LSI model to X and perform dimensionality reduction on X.\n",
    "reducedTrunc_labSVD = labSVD.fit_transform(lab_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standLabSVD = TruncatedSVD(n_components = standLab_k)\n",
    "standLabSVD.fit(standardizedLabMatrix)\n",
    "#Fit LSI model on training data X\n",
    "truncated_standLabSVD = standLabSVD.transform(standardizedLabMatrix)\n",
    "#Fit LSI model to X and perform dimensionality reduction on X.\n",
    "reducedTrunc_standLabSVD = standLabSVD.fit_transform(standardizedLabMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The explained variance for the lab data's  original LSI model is: \", truncated_labSVD.explained_variance_ratio)\n",
    "print(\"The explained variance for the lab data's model with dimensionality reduction is: \" \n",
    "      reducedTrunc_LabSVD.explained_variance_ratio)\n",
    "\n",
    "print(\"The explained variance for the standardized lab data's  original LSI model is: \", \n",
    "      truncated_standLabSVD.explained_variance_ratio)\n",
    "print(\"The explained variance for the standardized lab data's model with dimensionality reduction is: \" \n",
    "      reducedTrunc_standLabSVD.explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_approxDiff = np.subtract(lab_matrix, truncated_labSVD)\n",
    "lab_redApproxDiff = np.subtract(lab_matrix, reducedTrun_labSVD)\n",
    "\n",
    "print(\"The frobenius norm of the difference is: \", LA.norm(lab_approxDiff))\n",
    "print(\"The frobenius norm of the reduced difference is: \" LA.norm(lab_redApproxDiff))\n",
    "\n",
    "standLab_approxDiff = np.subtract(standardizedLabMatrix, truncated_standLabSVD)\n",
    "standLab_redApproxDiff = np.subtract(standardizedLabMatrix, reducedTrun_standLabSVD)\n",
    "\n",
    "print(\"The frobenius norm of the difference is: \", LA.norm(standLab_approxDiff))\n",
    "print(\"The frobenius norm of the reduced difference is: \" LA.norm(standLab_redApproxDiff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following these results, I decided to use my results to apply the k-means clustering algorithm tos ee if any further groupings in the data could be detected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Clustering\n",
    "\n",
    "K-means clustering was performed using sklearn's built in clustering functions. Optimal values for k were chosen given a variety of documented \"tests\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Kmeans package from sklearn \n",
    "from sklearn.cluster import KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate elbow plot to detect optimal number of clusters \n",
    "from scipy import cluster\n",
    "cluster_array = [cluster.vq.kmeans(results, i) for i in range(1,10)]\n",
    "\n",
    "plt.plot([var for (cent,var) in cluster_array])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert lab results to be stored in a pandas dataframe\n",
    "truncatedLab_df = pd.DataFrame(truncated_LabSVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K means Clustering method using sklearn functions \n",
    "def doKmeans(X, nclust=2):\n",
    "    model = KMeans(nclust)\n",
    "    model.fit(X)\n",
    "    clust_labels = model.predict(X)\n",
    "    cent = model.cluster_centers_\n",
    "    return (clust_labels, cent)\n",
    "\n",
    "clust_labels, cent = doKmeans(truncatedLab_df, 2)\n",
    "kmeans = pd.DataFrame(clust_labels)\n",
    "truncatedLab_df.insert((truncatedLab_df.shape[1]),'kmeans',kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the clusters obtained using k means\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(truncatedLab_df[0],truncatedLab_df[1],\n",
    "                     c=kmeans[0],s=50)\n",
    "plt.colorbar(scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_master_df = pd.read_csv(\"C:/Users/rachh/OneDrive/Documents/Senior Thesis/nmf_master_df.to_csv\", low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_lab_data = lab_data.apply(lambda x: x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size = 22\n",
    "corr = lab_data.corr()\n",
    "fig, ax = plt.subplots(figsize=(size, size))\n",
    "ax.matshow(corr)\n",
    "plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "plt.yticks(range(len(corr.columns)), corr.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(0)\n",
    "df = lab_data\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizedLabMatrix = standardizedLab_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply numpys SVD function (reduced version due to memory constraints)\n",
    "redLab_U, redLab_s, redLab_v = np.linalg.svd(standardizedLabMatrix,full_matrices = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sddLab_U, sddLab_s, sddLab_v = np.linalg.svd(sdd_labMatrix,full_matrices = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"C:/Users/rachh/Downloads/sddLabMatrix.csv\", results, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Master Matrix Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Data Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate matrix B for original matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export approximated matrix and store in .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_transformed = svd.fit_transform(master_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Clean Data Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_svd = TruncatedSVD(n_components = 5)\n",
    "lab_svd.fit(standardizedLab_matrix)\n",
    "results = lab_svd.transform(standardizedLab_matrix)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdd_lab_svd = TruncatedSVD(n_components = 2)\n",
    "sdd_lab_svd.fit(sdd_labMatrix)\n",
    "sddLab_results = sdd_lab_svd.transform(sdd_labMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"C:/Users/rachh/Downloads/sdd_lab_svd.csv\",sddLab_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SDD on truncated matrix produced by SVD - the logic behind this is that the SVD is denoising the matrix so that the \n",
    "#SVD can better detect the structure within it \n",
    "#labels from early columns of X can be used as labels for the clusters that the SDD finds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nmf we don't want to replace missing values, can accept sparse matrices as input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing NMF\n",
    "\n",
    "Now we will implement the NMF algorithms on the respective datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine number of components for NMF lab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_matrix = lab_data.values\n",
    "nmf_lab_matrix = nmf_lab_data.values\n",
    "U,s,V = np.linalg.svd(nmf_lab_matrix,full_matrices = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_lab_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,23),s)\n",
    "plt.title('Scree Plot of Singular Values: NMF Lab Data')\n",
    "plt.xticks(range(0,23))\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateKnee(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=3, init='random', random_state=0)\n",
    "W = model.fit_transform(nmf_lab_matrix)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(lab_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maternity and Ventilation Analysis\n",
    "\n",
    "Given literature, it seemed beneficial to conduct a targeted cohort analysis. I will apply a similar process as performed above on my new dataset: Maternity and Ventilation patients. This dataset includes lab data for patients who had Maternity and ventilation procedure codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maternity_df = pd.read_csv(\"C:/Users/rachh/OneDrive/Documents/Senior Thesis/maternity_df.csv\", low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maternity_matrix = maternity_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maternity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply numpy SVD function to master matrix\n",
    "maternity_U,maternity_s,maternity_V = np.linalg.svd(maternity_matrix, full_matrices = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maternity_svd = TruncatedSVD(n_components = 2)\n",
    "maternity_svd.fit(maternity_matrix)\n",
    "matSVD_results = maternity_svd.transform(maternity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform kmeans on maternity data \n",
    "matSVD_df = pd.DataFrame(matSVD_results)\n",
    "#K means Clustering \n",
    "clust_labels, cent = doKmeans(matSVD_df, 2)\n",
    "kmeans = pd.DataFrame(clust_labels)\n",
    "matSVD_df.insert((matSVD_df.shape[1]),'kmeans',kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matSVD_matrix = matSVD_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have clustered the SVD results, we will perform NMF on the maternity dataframe and see how our results differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_data = maternity_df.drop(columns = ['Unnamed: 0'])\n",
    "mat_data = mat_data.drop(columns = ['dischtime'])\n",
    "mat_data = mat_data.drop(columns = ['dob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_matrix = mat_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_model = NMF(n_components=None, init='random', random_state=0)\n",
    "mat_W = mat_model.fit_transform(mat_matrix)\n",
    "mat_H = mat_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(mat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(mat_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(mat_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matNMF_df = mat_W @ mat_H\n",
    "matNMF_df = pd.DataFrame(matNMF_df)\n",
    "#perform clustering \n",
    "clust_labels, cent = doKmeans(matNMF_df, 2)\n",
    "kmeans = pd.DataFrame(clust_labels)\n",
    "matNMF_df.insert((matNMF_df.shape[1]),'kmeans',kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF and SVD Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTruncatedSVD(A):\n",
    "    #apply numpy SVD function to input matrix\n",
    "    k = calculateK(A)\n",
    "    svd = TruncatedSVD(n_components = k)\n",
    "    svd.fit(A)\n",
    "    results = svd.transform(A)\n",
    "    return results\n",
    "\n",
    "def calculateK(A):\n",
    "    #compute basic svd to find K value\n",
    "    U,s,V = np.linalg.svd(A,full_matrices = False)\n",
    "    k = calculateKnee(s)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNMFComponents(A):\n",
    "    #apply sklearn NMF function to input matrix\n",
    "    k = calculateK(A)\n",
    "    mat_model = NMF(n_components=k)\n",
    "    mat_W = mat_model.fit_transform(A)\n",
    "    mat_H = mat_model.components_\n",
    "    return mat_W, mat_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNMF(A):\n",
    "   #apply sklearn NMF function to input matrix\n",
    "    k = calculateK(A)\n",
    "    mat_model = NMF(n_components=k)\n",
    "    mat_W = mat_model.fit_transform(A)\n",
    "    mat_H = mat_model.components_\n",
    "    return mat_W @ mat_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Data Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labMatrix = lab_data.values\n",
    "nmf_labMatrix = nmf_lab_data.values\n",
    "sdd_labMatrix = sdd_lab_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdLab_results = computeTruncatedSVD(labMatrix)\n",
    "sddLab_results = computeTruncatedSVD(sdd_labMatrix)\n",
    "nmfLab_results = computeNMF(nmf_labMatrix)\n",
    "\n",
    "svdLab_resultsDF = pd.DataFrame(svdLab_results)\n",
    "sddLab_resultsDF = pd.DataFrame(sddLab_results)\n",
    "nmfLab_resultsDF = pd.DataFrame(nmfLab_results)\n",
    "\n",
    "svdLab_resultsDF.to_csv(\"C:/Users/rachh/OneDrive/Documents/Senior Thesis/svdLab_results.csv\")\n",
    "sddLab_resultsDF.to_csv(\"C:/Users/rachh/OneDrive/Documents/Senior Thesis/sddLab_results.csv\")\n",
    "nmfLab_resultsDF.to_csv(\"C:/Users/rachh/OneDrive/Documents/Senior Thesis/nmfLab_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmfLabW, nmfLabH = computeNMFComponents(nmf_lab_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Master Matrix Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmSVD_results = computeTruncatedSVD(master_matrix)\n",
    "mmNMF_results = computeNMF(master_matrix)\n",
    "\n",
    "mmSVD_resultsDF = pd.DataFrame(mmSVD_results)\n",
    "mmNMF_resultsDF = pd.DataFrame(mmNMF_results)\n",
    "\n",
    "mmSVD_resultsDF.to_csv(\"C:/Users/rachh/OneDrive/Documents/Senior Thesis/mmSVD_results.csv\")\n",
    "mmNMF_resultsDF.to_csv(\"C:/Users/rachh/OneDrive/Documents/Senior Thesis/mmNMF_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
